{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5), ('gutenberg', 4), ('to', 4), ('you', 4), ('project', 3), ('about', 3), ('contact', 3), ('of', 3), ('and', 3), ('help', 3)]\n"
     ]
    }
   ],
   "source": [
    "#1 Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_most_frequent_words(url, topno=10):\n",
    "    \n",
    "    response = requests.get(url) # Fetch content from the URL\n",
    "    content = response.text\n",
    "\n",
    "    #we will need BeautifulSoup for HTML parsing\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "   \n",
    "    text = soup.get_text()  # Extracting text from the HTML\n",
    "\n",
    "   \n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Use regex to find words\n",
    "\n",
    "    word_counts = Counter(words)  # Count occurrences of each word\n",
    "   \n",
    "\n",
    "    most_frequent_words = word_counts.most_common(topno)  # Get the top N most frequent words\n",
    "\n",
    "    return most_frequent_words\n",
    "\n",
    "\n",
    "romeo_and_juliet_url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "# 10 most frequent words\n",
    "most_frequent_words = get_most_frequent_words(romeo_and_juliet_url, 10)\n",
    "\n",
    "\n",
    "print(most_frequent_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['weight', 'id', 'name', 'cfa_url', 'vetstreet_url', 'vcahospitals_url',\n",
      "       'temperament', 'origin', 'country_codes', 'country_code', 'description',\n",
      "       'life_span', 'indoor', 'lap', 'alt_names', 'adaptability',\n",
      "       'affection_level', 'child_friendly', 'dog_friendly', 'energy_level',\n",
      "       'grooming', 'health_issues', 'intelligence', 'shedding_level',\n",
      "       'social_needs', 'stranger_friendly', 'vocalisation', 'experimental',\n",
      "       'hairless', 'natural', 'rare', 'rex', 'suppressed_tail', 'short_legs',\n",
      "       'wikipedia_url', 'hypoallergenic', 'reference_image_id', 'cat_friendly',\n",
      "       'bidability'],\n",
      "      dtype='object')\n",
      "Statistics for Cats' Weight (in metric units):\n",
      "{'min': nan, 'max': nan, 'mean': nan, 'median': nan, 'std_dev': nan}\n",
      "\n",
      "Statistics for Cats' Lifespan (in years):\n",
      "{'min': 8, 'max': 18, 'mean': 12.074626865671641, 'median': 12.0, 'std_dev': 1.8283411328456125}\n",
      "\n",
      "Frequency Table of Country and Breed:\n",
      "           origin              name  count\n",
      "0       Australia   Australian Mist      1\n",
      "1           Burma           Burmese      1\n",
      "2           Burma  European Burmese      1\n",
      "3          Canada            Cymric      1\n",
      "4          Canada            Sphynx      1\n",
      "..            ...               ...    ...\n",
      "62  United States          Savannah      1\n",
      "63  United States       Selkirk Rex      1\n",
      "64  United States          Snowshoe      1\n",
      "65  United States            Toyger      1\n",
      "66  United States    York Chocolate      1\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#2 Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :\n",
    "#the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "#the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "#Create a frequency table of country and breed of cats\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "\n",
    "response = requests.get(cats_api) # Fetch data from the Cat API\n",
    "breeds_data = response.json()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(breeds_data) # Create a DataFrame from the API response\n",
    "\n",
    "\n",
    "print(df.columns) # Check the columns in the DataFrame\n",
    "\n",
    "# Convert weight and lifespan to numeric values\n",
    "# Convert weight and lifespan to numeric values\n",
    "df['weight.metric'] = pd.to_numeric(df['weight'], errors='coerce') * 0.453592  # this convert pounds to kilograms\n",
    "df['life_span'] = pd.to_numeric(df['life_span'].str.extract('(\\d+)')[0], errors='coerce')  # this extract only numeric values\n",
    "\n",
    "# Calculate statistics for weight\n",
    "weight_stats = {\n",
    "    'min': df['weight.metric'].min(),\n",
    "    'max': df['weight.metric'].max(),\n",
    "    'mean': df['weight.metric'].mean(),\n",
    "    'median': df['weight.metric'].median(),\n",
    "    'std_dev': df['weight.metric'].std()\n",
    "}\n",
    "\n",
    "# Calculate statistics for lifespan\n",
    "lifespan_stats = {\n",
    "    'min': df['life_span'].min(),\n",
    "    'max': df['life_span'].max(),\n",
    "    'mean': df['life_span'].mean(),\n",
    "    'median': df['life_span'].median(),\n",
    "    'std_dev': df['life_span'].std()\n",
    "}\n",
    "\n",
    "# Creating a frequency table\n",
    "frequency_table = df.groupby(['origin', 'name']).size().reset_index(name='count')\n",
    "\n",
    "\n",
    "print(\"Statistics for Cats' Weight (in metric units):\")\n",
    "print(weight_stats)\n",
    "\n",
    "print(\"\\nStatistics for Cats' Lifespan (in years):\")\n",
    "print(lifespan_stats)\n",
    "\n",
    "print(\"\\nFrequency Table of Country and Breed:\")\n",
    "print(frequency_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Largest Countries:\n",
      "                         name        area\n",
      "185        Russian Federation  17124442.0\n",
      "8                  Antarctica  14000000.0\n",
      "42                     Canada   9984670.0\n",
      "48                      China   9640011.0\n",
      "239  United States of America   9629091.0\n",
      "31                     Brazil   8515767.0\n",
      "13                  Australia   7692024.0\n",
      "104                     India   3287590.0\n",
      "10                  Argentina   2780400.0\n",
      "117                Kazakhstan   2724900.0\n",
      "\n",
      "10 Most Spoken Languages:\n",
      "English       91\n",
      "French        45\n",
      "Arabic        25\n",
      "Spanish       24\n",
      "Portuguese    10\n",
      "Russian        8\n",
      "Dutch          8\n",
      "German         7\n",
      "Chinese        5\n",
      "Italian        4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Number of Languages:\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "#3 Read the countries API and find\n",
    "#the 10 largest countries\n",
    "#the 10 most spoken languages\n",
    "#the total number of languages in the countries API\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "countries_api = 'https://restcountries.com/v2/all' # Fetch data from the Countries API\n",
    "response = requests.get(countries_api)\n",
    "countries_data = response.json()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(countries_data) # Create a DataFrame from the API response\n",
    "\n",
    "# Convert area and population to numeric values\n",
    "df['area'] = pd.to_numeric(df['area'], errors='coerce') \n",
    "\n",
    "df['population'] = pd.to_numeric(df['population'], errors='coerce')\n",
    "\n",
    "#10 largest countries\n",
    "largest_countries = df.nlargest(10, 'area')[['name', 'area']]\n",
    "print(\"10 Largest Countries:\")\n",
    "print(largest_countries)\n",
    "\n",
    "#10 most spoken languages\n",
    "languages = [lang['name'] for country in countries_data for lang in country.get('languages', [])]\n",
    "language_counts = pd.Series(languages).value_counts().head(10)\n",
    "print(\"\\n10 Most Spoken Languages:\")\n",
    "print(language_counts)\n",
    "\n",
    "#total number of languages in the countries API\n",
    "total_languages = pd.Series(languages).nunique()\n",
    "print(\"\\nTotal Number of Languages:\")\n",
    "print(total_languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html data-theme=\"light\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"Discover datasets around the world!\" name=\"description\"/>\n",
      "  <link href=\"/favicon.ico\" rel=\"icon\"/>\n",
      "  <link href=\"/apple-touch-icon.png\" rel=\"apple-touch-icon\"/>\n",
      "  <!-- Provides metadata used when the app is installed on a m\n"
     ]
    }
   ],
   "source": [
    "#4 UCI is one of the most common places to get data sets for data science and machine learning. \n",
    "#Read the content of UCL (https://archive.ics.uci.edu/ml/datasets.php). Without additional libraries it will be difficult, so you may try it with BeautifulSoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "uci_url = 'https://archive.ics.uci.edu/ml/datasets.php' # URL of the UCI Machine Learning Repository\n",
    "\n",
    "\n",
    "response = requests.get(uci_url) # Fetch the HTML content from the URL\n",
    "html_content = response.text\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser') # Parse the HTML using BeautifulSoup\n",
    "\n",
    "# Extract and print the 1st 400content\n",
    "print(soup.prettify()[:400])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
